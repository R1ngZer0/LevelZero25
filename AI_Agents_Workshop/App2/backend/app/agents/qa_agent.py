"""
Implements the Reflection/QA Agent logic.

This agent receives content generated by other agents and evaluates it for quality,
relevance, and accuracy, providing feedback or approval.
"""

from typing import List, Optional
from pydantic import BaseModel, Field
# from langchain.agents import Agent # Potential alternative?
# from langchain.tools import Tool # Potential alternative?
from pydantic_ai import Agent # Restore PydanticAI import
from app.schemas.qa_schemas import QAInput, QAOutput
from app.llm_clients import get_llm_client
import logging

# Placeholder for a more sophisticated prompt or configuration
QA_SYSTEM_PROMPT = (
    "You are a meticulous Quality Assurance agent. Your role is to review the provided content. "
    "Assess if the content accurately addresses the original prompt (if provided), aligns with the given context (if provided), "
    "and meets general quality standards (clarity, coherence, factual accuracy). "
    "Based on your assessment, decide if the content is approved. If not approved, provide specific, constructive feedback for revision."
    "Output ONLY the QAOutput object."
)

logger = logging.getLogger(__name__)

def review_content(input_data: QAInput) -> QAOutput:
    """Reviews generated content using the configured OpenAI LLM."""
    logger.info(f"Starting QA review for prompt: {input_data.original_prompt[:50]}...")
    llm_client = get_llm_client()
    
    # Combine input data into a structured request for the LLM
    review_request = f"Original Prompt: {input_data.original_prompt}\nContext: {input_data.context}\n\nContent to Review:\n{input_data.content_to_review}"

    # Revert to using pydantic_ai.Agent
    qa_evaluator = Agent(
        llm=llm_client,
        system_prompt=QA_SYSTEM_PROMPT,
        output_model=QAOutput
    )

    try:
        # Use invoke to get the structured output from pydantic_ai.Agent
        qa_result: QAOutput = qa_evaluator.invoke(review_request)
        logger.info(f"QA Result: Approved={qa_result.is_approved}, Feedback='{qa_result.feedback[:100]}...'")
        return qa_result
    except Exception as e:
        logger.error(f"Error during QA evaluation: {e}", exc_info=True)
        # Handle potential errors during LLM invocation or parsing
        print(f"Error during QA evaluation: {e}")
        # Fallback or default response
        return QAOutput(is_approved=False, feedback=f"Evaluation failed due to an internal error: {e}")

# Example Usage (for testing purposes)
if __name__ == '__main__':
    test_input_approved = QAInput(
        content_to_review="The capital of France is Paris.",
        original_prompt="What is the capital of France?",
        context="General knowledge query."
    )
    result_approved = review_content(test_input_approved)
    print("Approved Test Case:", result_approved)

    test_input_needs_revision = QAInput(
        content_to_review="Paris is the capital of France, a city known for its baguettes.",
        original_prompt="Provide a detailed description of the Eiffel Tower.",
        context="Document about French landmarks."
    )
    result_revision = review_content(test_input_needs_revision)
    print("Revision Test Case:", result_revision) 