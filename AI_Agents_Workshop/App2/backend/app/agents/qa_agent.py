"""
Implements the Reflection/QA Agent logic.

This agent receives content generated by other agents and evaluates it for quality,
relevance, and accuracy, providing feedback or approval.
"""

from pydantic_ai import PydanticAI
from app.schemas.qa_schemas import QAInput, QAOutput
from app.llm_clients import get_llm_client

# Placeholder for a more sophisticated prompt or configuration
QA_SYSTEM_PROMPT = (
    "You are a meticulous Quality Assurance agent. Your role is to review the provided content. "
    "Assess if the content accurately addresses the original prompt (if provided), aligns with the given context (if provided), "
    "and meets general quality standards (clarity, coherence, factual accuracy). "
    "Based on your assessment, decide if the content is approved. If not approved, provide specific, constructive feedback for revision."
    "Output ONLY the QAOutput object."
)

def review_content(input_data: QAInput, mode: str = "cloud") -> QAOutput:
    """Reviews generated content using an LLM based on the selected mode."""
    llm_client = get_llm_client(mode)
    
    # Combine input data into a structured request for the LLM
    review_request = f"Original Prompt: {input_data.original_prompt}\nContext: {input_data.context}\n\nContent to Review:\n{input_data.content_to_review}"

    qa_evaluator = PydanticAI(
        llm=llm_client,
        system_prompt=QA_SYSTEM_PROMPT,
        output_model=QAOutput
    )

    try:
        # Use invoke to get the structured output
        qa_result: QAOutput = qa_evaluator.invoke(review_request)
        return qa_result
    except Exception as e:
        # Handle potential errors during LLM invocation or parsing
        print(f"Error during QA evaluation: {e}")
        # Fallback or default response
        return QAOutput(is_approved=False, feedback=f"Evaluation failed due to an internal error: {e}")

# Example Usage (for testing purposes)
if __name__ == '__main__':
    test_input_approved = QAInput(
        content_to_review="The capital of France is Paris.",
        original_prompt="What is the capital of France?",
        context="General knowledge query."
    )
    result_approved = review_content(test_input_approved, mode="cloud") # Assuming OpenAI API key is set
    print("Approved Test Case:", result_approved)

    test_input_needs_revision = QAInput(
        content_to_review="Paris is the capital of France, a city known for its baguettes.",
        original_prompt="Provide a detailed description of the Eiffel Tower.",
        context="Document about French landmarks."
    )
    result_revision = review_content(test_input_needs_revision, mode="cloud") # Assuming OpenAI API key is set
    print("Revision Test Case:", result_revision) 